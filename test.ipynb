{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n",
    "# set UPSTAGE_API_KEY in .env file\n",
    "# UPSTAGE_API_KEY=your_api_key\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "We introduce SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters, \n",
    "demonstrating superior performance in various natural language processing (NLP) tasks. \n",
    "Inspired by recent efforts to efficiently up-scale LLMs, \n",
    "we present a method for scaling LLMs called depth up-scaling (DUS), \n",
    "which encompasses depthwise scaling and continued pretraining.\n",
    "In contrast to other LLM up-scaling methods that use mixture-of-experts, \n",
    "DUS does not require complex changes to train and inference efficiently. \n",
    "We show experimentally that DUS is simple yet effective \n",
    "in scaling up high-performance LLMs from small ones. \n",
    "Building on the DUS model, we additionally present SOLAR 10.7B-Instruct, \n",
    "a variant fine-tuned for instruction-following capabilities, \n",
    "surpassing Mixtral-8x7B-Instruct. \n",
    "SOLAR 10.7B is publicly available under the Apache 2.0 license, \n",
    "promoting broad access and application in the LLM field.\n",
    "\"\"\"\n",
    "\n",
    "query = f\"\"\"\n",
    "Make 3 questions using this text.\n",
    "\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "query2 = \"\"\"\n",
    "Linear transformation is a function that satisfies two features.\n",
    "One is homogeneity, and other is additivity.\n",
    "\n",
    "Linear transformation of the subspace is also subspace.\n",
    "Prove or disprove this statement based on given contents.\n",
    "\"\"\"\n",
    "\n",
    "query3 = \"\"\"\n",
    "The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
    "\"\"\"\n",
    "\n",
    "query4 = \"\"\"\n",
    "What is a Mahjong game?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "카이스트 전산학부 건물인 신뢰관에 대한 정보를 제공해주세요.\n",
      "신뢰관은 카이스트 전산학부의 건물 중 하나입니다. 신뢰관의 구체적인 정보나 특징, 역사 등에 대해 알고 싶으신 경우, 신뢰관과 관련된 내용을 알려주시면 해당 정보를 제공해드릴 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_upstage import ChatUpstage\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatUpstage()\n",
    "\n",
    "message = query5\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are the AI that separates instruction and context from the query.\n",
    "In this query, separate the instruction and context. Instruction and context must not have duplicate contents.\n",
    "Only use given query itself, do not generate or change any words.\n",
    "Instruction is saying about what AI should do.\n",
    "Context is can be a base information or some text to convert or manipulate.\n",
    "Context can be not provided, then print context to space.\n",
    "Print instruction as first paragraph and context as second paragraph.\n",
    "---\n",
    "Query : {query}\n",
    "\"\"\"\n",
    ")\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "answer = chain.invoke({\"query\": message})\n",
    "\n",
    "instr, context = [s.strip() for s in answer.split(\"Instruction:\")[1].split(\"Context:\")]\n",
    "print(instr)\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. \"카이스트 전산학부 건물인 신뢰관\"을 검색합니다.', '2. 검색 결과에서 신뢰관의 구체적인 정보, 특징, 역사 등을 확인합니다.', '3. 검색 결과에서 신뢰관의 관련 이미지나 링크를 찾습니다.', '4. 신뢰관의 구체적인 정보가 검색 결과에서 나오지 않는다면, \"카이스트 전산학부 신뢰관\"을 검색합니다.', '5. 검색 결과에서 신뢰관의 구체적인 정보가 나오지 않는다면, \"KAIST 전산학부 신뢰관\"을 검색합니다.', '6. 검색 결과에서 신뢰관의 구체적인 정보가 나오지 않는다면, \"KAIST 전산학부 건물 신뢰관\"을 검색합니다.', '7. 검색 결과에서 신뢰관의 구체적인 정보가 나오지 않는다면, \"KAIST 전산학부 신뢰관\"을 검색합니다.', '8. 검색 결과에서 신뢰관의 구체적인 정보가 나오지 않는다면, \"KAIST 전산학부 신뢰관 역사\"를 검색합니다.', '9. 검색 결과에서 신뢰관의 구체적인 정보가 나오지 않는다면, \"KAIST 전산학부 신뢰관 특징\"을 검색합니다.', '10. 검색 결과에서 신뢰관의 구체적인 정보가 나오지 않는다면, \"KAIST 전산학부 신뢰관 크기\"를 검색합니다.', '11. 검색 결과에서 신뢰관의 구체적인 정보가 나오지 않는다면, \"KAIST 전산학부 신뢰관 구조\"를 검색합니다.', '12. 검색 결과에서 신뢰관의 구체적인 정보가 나오지 않는다면, \"KAIST 전산학부 신뢰관 역사\"를 검색합니다.']\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tavily import TavilyClient\n",
    "\n",
    "tavily = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
    "\n",
    "scontext = str(tavily.search(query=instr))\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "For instruction \"{instr}\", divide instruction into few steps.\n",
    "Do not answer or calculate for each steps, and only divide instructions to the smaller problem.\n",
    "Only print 1., 2., 3., ... steps to do. Each step should be a one-liner.\n",
    "Refer to the context if you need : {context}\n",
    "Searched context to help dividing instructions : {scontext}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "answer = chain.invoke({\"instr\": instr, \"context\": context, \"scontext\": scontext}).split(\"\\n\")\n",
    "print(answer)\n",
    "print(len(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "네.\n"
     ]
    }
   ],
   "source": [
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "For instruction \"{instr}\", determine instruction can be done only using context.\n",
    "Just answer it in \"Yes\" or \"No\".\n",
    "---\n",
    "Context : {context}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "answer = chain.invoke({\"instr\": instr, \"context\": context})\n",
    "print(answer)\n",
    "\n",
    "add = \"\"\n",
    "\n",
    "if(answer == \"No\"): # find context from tavily\n",
    "    context = scontext\n",
    "    add = \"\\nAnswer with references if it exists.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "rag_with_history_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = rag_with_history_prompt | llm | StrOutputParser()\n",
    "\n",
    "history = []\n",
    "\n",
    "for i in range(len(answer)):\n",
    "    if (i == 0 and len(context) != 0):\n",
    "        history.append(\n",
    "            HumanMessage(answer[i] + \"\\n Refer to or use this context.\\n Context:\" + context)\n",
    "        )\n",
    "    else:\n",
    "        history.append(\n",
    "            HumanMessage(answer[i]) \n",
    "        )\n",
    "    chain_result = chain.invoke({\"history\": history})\n",
    "    history.append(\n",
    "        AIMessage(chain_result)\n",
    "    )\n",
    "\n",
    "history.append(\n",
    "    HumanMessage(message + add + \"\\nIf answer is not context-based, then only print 'I don't know about that.'\")\n",
    ")\n",
    "answer = chain_result = chain.invoke({\"history\": history})\n",
    "\n",
    "print(answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
