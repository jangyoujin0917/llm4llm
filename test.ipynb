{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n",
    "# set UPSTAGE_API_KEY in .env file\n",
    "# UPSTAGE_API_KEY=your_api_key\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "We introduce SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters, \n",
    "demonstrating superior performance in various natural language processing (NLP) tasks. \n",
    "Inspired by recent efforts to efficiently up-scale LLMs, \n",
    "we present a method for scaling LLMs called depth up-scaling (DUS), \n",
    "which encompasses depthwise scaling and continued pretraining.\n",
    "In contrast to other LLM up-scaling methods that use mixture-of-experts, \n",
    "DUS does not require complex changes to train and inference efficiently. \n",
    "We show experimentally that DUS is simple yet effective \n",
    "in scaling up high-performance LLMs from small ones. \n",
    "Building on the DUS model, we additionally present SOLAR 10.7B-Instruct, \n",
    "a variant fine-tuned for instruction-following capabilities, \n",
    "surpassing Mixtral-8x7B-Instruct. \n",
    "SOLAR 10.7B is publicly available under the Apache 2.0 license, \n",
    "promoting broad access and application in the LLM field.\n",
    "\"\"\"\n",
    "\n",
    "query = f\"\"\"\n",
    "Make 3 questions using this text.\n",
    "\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "query2 = \"\"\"\n",
    "Linear transformation is a function that satisfies two features.\n",
    "One is homogenity, and other is additivity.\n",
    "\n",
    "Linear transformation of the subspace is also subspace.\n",
    "Prove or disprove this statement based on given contents.\n",
    "\"\"\"\n",
    "\n",
    "query3 = \"\"\"\n",
    "The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
    "\"\"\"\n",
    "\n",
    "query4 = \"\"\"\n",
    "What is a Mahjong game?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The instruction is asking the AI to prove or disprove the statement \"Linear transformation of the subspace is also subspace\" based on the given contents.\n",
      "The context provides information about linear transformation and its features. It states that linear transformation is a function that satisfies two features: homogeneity and additivity.\n",
      "\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from langchain_upstage import ChatUpstage\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatUpstage()\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "In this query, separate the instruction and context.\n",
    "Do not answer to the query, just separate it. Also, only use given query itself, do not generate or change any words.\n",
    "Instruction is saying about what AI should do.\n",
    "Context is can be a base information or some text to convert or manipulate.\n",
    "Context can be not provided(N/A for this case).\n",
    "Instruction and context must not have duplicate contents.\n",
    "Print instruction as first paragraph and context as second paragraph.\n",
    "---\n",
    "Query : {query}\n",
    "\"\"\"\n",
    ")\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "answer = chain.invoke({\"query\": query2})\n",
    "\n",
    "instr, context = [s.strip() for s in answer.split(\"Instruction:\")[1].split(\"Context:\")]\n",
    "print(instr)\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. Understand the definition of a subspace.', '2. Understand the definition of a linear transformation.', '3. Apply the definition of a linear transformation to a subspace.', '4. Check if the result of the transformation is still a subspace.', '5. Compare the result with the original subspace.', '6. Determine if the transformation preserves the subspace property.', '7. Conclude whether the statement \"Linear transformation of the subspace is also subspace\" is true or false based on the results.']\n"
     ]
    }
   ],
   "source": [
    "# 각 chain을 실행하고 결과 누적으로 얻음\n",
    "# (전체 히스토리 누적 후) 최종 결과 질문\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "For instruction \"{instr}\", give me what to do to achieve this instruction for AI.\n",
    "Only print 1., 2., 3., ... steps to do. Do not answer to the question, and only print steps.\n",
    "Each step should be a one-liner.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "answer = chain.invoke({\"instr\": instr, \"context\": context}).split(\"\\n\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To prove or disprove the statement \"Linear transformation of the subspace is also subspace,\" we need to consider the properties of linear transformations and subspaces.\n",
      "\n",
      "A linear transformation T: V → W, where V and W are vector spaces over the same field F, is a function that satisfies two properties:\n",
      "\n",
      "1. Homogeneity: T(αv) = αT(v) for all α ∈ F and v ∈ V.\n",
      "2. Additivity: T(v1 + v2) = T(v1) + T(v2) for all v1, v2 ∈ V.\n",
      "\n",
      "A subspace U of a vector space V is a subset of V that satisfies three properties:\n",
      "\n",
      "1. Closure under vector addition: For all u1, u2 ∈ U, u1 + u2 ∈ U.\n",
      "2. Closure under scalar multiplication: For all α ∈ F and u ∈ U, αu ∈ U.\n",
      "3. Contains the zero vector of V: There exists a vector 0 ∈ U such that for any u ∈ U, u + 0 = u.\n",
      "\n",
      "Now, let's consider the image of a subspace U under a linear transformation T: T(U) = {T(u) : u ∈ U}.\n",
      "\n",
      "1. Closure under vector addition: Since T is additive, T(u1 + u2) = T(u1) + T(u2) for all u1, u2 ∈ U. This means that the image of the sum of two vectors in U is the sum of their images in W. Therefore, T(u1 + u2) ∈ T(U), and T(U) is closed under vector addition.\n",
      "\n",
      "2. Closure under scalar multiplication: Since T is homogeneous, T(αu) = αT(u) for all α ∈ F and u ∈ U. This means that the image of a scalar multiple of a vector in U is the scalar multiple of its image in W. Therefore, T(αu) ∈ T(U), and T(U) is closed under scalar multiplication.\n",
      "\n",
      "3. Contains the zero vector: Since U contains the zero vector of V, and T is a linear transformation, we have T(0U) = T(0V) = 0W, where 0U is the zero vector of U and 0V is the zero vector of V. Therefore, T(U) contains the zero vector of W.\n",
      "\n",
      "Based on these properties, we can conclude that the image of a subspace under a linear transformation is also a subspace of the target vector space. Therefore, the statement \"Linear transformation of the subspace is also subspace\" is true.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "rag_with_history_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = rag_with_history_prompt | llm | StrOutputParser()\n",
    "\n",
    "history = []\n",
    "\n",
    "for i in range(len(answer)):\n",
    "    if (i == 0):\n",
    "        history.append(\n",
    "            HumanMessage(answer[i] + \"\\n Refer to or use this context.\\n Context:\" + context)\n",
    "        )\n",
    "    else:\n",
    "        history.append(\n",
    "            HumanMessage(answer[i]) \n",
    "        )\n",
    "    chain_result = chain.invoke({\"history\": history})\n",
    "    history.append(\n",
    "        AIMessage(chain_result)\n",
    "    )\n",
    "\n",
    "history.append(\n",
    "    HumanMessage(instr)\n",
    ")\n",
    "answer = chain_result = chain.invoke({\"history\": history})\n",
    "print(answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
